---
title: "Predicting and Inferring Graduation Rate Among U.S. Colleges"
output: html_notebook
---
<h1 style="font-size: 30px;">Introduction</h1>

For my project, I plan to work with the College dataset located in the ISLR2 library created by the authors of the book "An Introduction to Statistical Learning with Applications in R". This dataset was collected by sampling information from 777 colleges from the 1995 issue of US News and World Report which is a news magazine that publishes rankings of colleges within the US. The purpose of collecting this data was to provide a resource for students and researchers to explore the correlation between different factors and the performance of US colleges. Each college is represented in 18 different columns containing statistics on various aspects of that college which include:

1. Private: A factor indicating whether the college is private or public.
2. Apps: The number of applications received by the college.
3. Accept: The number of applications accepted by the college.
4. Enroll: The number of students enrolled at the college.
5. Top10perc: The percentage of students at the college who scored in the top 10% of their high school classes.
6. Top25perc: The percentage of students at the college who scored in the top 25% of their high school classes.
7. F.Undergrad: The number of full time undergraduates at the college.
8. P.Undergrad: The number of part time undergraduates at the college.
9. Outstate: The out-of-state tuition for the college.
10. Room.Board: The cost of room and board at the college.
11. Books: The estimated cost of books for a year of study at the college.
12. Personal: The estimated personal expenses for a year of study at the college.
13. PhD: The percentage of faculty at the college with PhDs.
14. Terminal: The percentage of faculty at the college with terminal degrees
15. S.F.Ratio: The student to faculty ratio at the college.
16. perc.alumni: The percentage of alumni who donate to the college.
17. Expend: The instructional expenditure per student at the college.
18. Grad.Rate: The graduation rate for the college.

Some representation decisions were made when creating this dataset. For example, the Private column was coded as a factor with two levels, "Yes" and "No," rather than a numerical value. This decision was made to avoid issues with comparing private and public colleges based on cost, admission rates, or other factors. Also, the Ph.D. and Terminal columns were represented as percentages rather than numbers to provide a better representation of the faculty's qualifications.

This project will provide valuable insights into the correlation between different factors and the performance of US colleges. Through the use of statistical analysis and machine learning techniques, I aim to determine which variables are most significant and provide recommendations for students who are seeking to select the best college for their needs.

<h1 style="font-size: 30px;">Questions</h1>

My objective is to explore what factors are most correlated with a college's graduation rate as well as to be able to predict accurately a new colleges graduation rate given some data. This analysis will offer insights into the state of higher education in the US and identify what is being paid for. The results of my study could be useful for others who are looking to choose the best college for their needs.

<h1 style="font-size: 30px;">Visualization</h1>

To understand and gain insights into patterns and trends within the data that are not immediately visible its important to visualize our data. We first can load up all of our libraries we will need and convert all the quantitative data into qualitative using this and saving it into the College_data variable.

```{r}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(glmnet)
library(caret)
library(Metrics)
library(rsq)

College_data = College_quant_data <- na.omit(College) %>%
  mutate_all(~ if(is.factor(.)) {
    as.numeric(as.factor(.))
  } else {
    .
  })
```

We can then start to visualize our data by creating a histogram of the graduation rate and a scatter plot matrices for each variable we are investigating between all other variables as well as listing their correlations.

```{r}

colnames = names(College_data)
message(sprintf("The mean of Grad.Rate is %f", mean(College_data$Grad.Rate)))
message(sprintf("The standard deviation of Grad.Rate is %f", sd(College_data$Grad.Rate)))

hist(College_data$Grad.Rate, col = "blue", main = "Graduation Rates")

par(mfrow = c(3, 3))
for (i in 2:length(colnames) - 1) {
  # make scatter plot of Grad.Rate against the current predictor
  plot(College[,i], College$Grad.Rate, xlab = colnames[i], ylab = "Grad.Rate")
}
cor(College_data)[- which(colnames(College) == "Grad.Rate"), "Grad.Rate"] # print out correlations except the grad to grad
```

Looking at the histogram we can see that it follows a Gaussian, centered around 60-70% with a mean of ~65.46, and a standard deviation of ~17.17 meaning there is a lot of spread with the data. It also has an outlier with a graduation rate more than 100% which obviously cant be the case as its caps at 100%.

Analyzing the scatter plots and the correlations we can see that the Private, Top10perc, Top20prec, Outstate, Room.Board, phD, S.F.Ratio, perc.alumni and Expend have evidence of being moderately correlated with the Graduation Rate. Using this information we can hypothesize before we fit our model that some of these variables can be related to Graduation rate.

<h1 style="font-size: 30px;">Modeling</h1>

To achieve my goal, I plan to first use a linear regression model to predict the quantitative responses. I will also experiment with using random forests and decision trees to get better results by trying to capture the more complex relationships in the data. I also plan to apply cross-validation and Lasso (in the linear regression case) to ensure the validity of my model. I will make two separate models for each model type one using a hold-out cross validation method and the other either using LOOCV or k-fold.

To start we divide the data to be 70% training and 30% testing.

```{r}
set.seed(0) 

alpha = 1
k = 10
lambda = 10^seq(10, -2, length = 100)

# Divide the data into training and test
x = model.matrix(Grad.Rate ~ ., data = College_data)[, -1]
y = College_data$Grad.Rate

trainIndex = createDataPartition(College_data$Grad.Rate, p = 0.7, list = FALSE)
training = na.omit(College_data[trainIndex, ])
testing = na.omit(College_data[-trainIndex, ])
```

<h1 style="font-size: 18px;">**Linear Regression**</h1>

Then using these two sets of data we can fit a linear regression model using k-folds and Lasso to predict the Graduation rate. I will also fit a hold-out cross-validated linear regression on the training data so we can infer the relationships between the predictors and response, and see how much the models performance improves using cross validation.

```{r}
#----------------------------------------------------------------
# Fit on training data
cv = trainControl(method = "cv", number = k)
shrink.mod = train(Grad.Rate ~ ., data = training, method = "glmnet", trControl = cv)

lm.fit = lm(Grad.Rate ~ ., data=training)

# glmnet is lasso
#----------------------------------------------
# Generalization Error

shrink.pred = predict(shrink.mod, testing)
MSE =  mean((testing$Grad.Rate - shrink.pred)^2)
Maek = mae(shrink.pred, testing$Grad.Rate)
Rsqk = R2(shrink.pred, testing$Grad.Rate)

shrink.pred = predict(lm.fit, testing)
MSElm =  mean((testing$Grad.Rate - shrink.pred)^2)
Maelm = mae(shrink.pred, testing$Grad.Rate)
Rsqlm = R2(shrink.pred, testing$Grad.Rate)



message("Metrics for for the decision linear regression with hold-out cross validation")
par(mfrow = c(2, 2))
plot(lm.fit)
print(varImp(lm.fit))
message(sprintf("Linear regression with hold-out cross validation statistic R^2 is %f", Rsqlm))
message(sprintf("Linear regression with hold-out cross validation statistic MAE is %f", Maelm))
message(sprintf("The Mean squared error of the linear regression with hold-out cross validation is %f", MSElm))

message("Metrics for for the Linear regression with k-fold cross validation and lasso fit")
summary(cv.fit$results)
plot(shrink.mod)
print(varImp(cv.fit$finalModel))
message(sprintf("Linear regression with k-fold cross validation and lasso statistic R^2 is %f", Rsqk))
message(sprintf("Linear regression with k-fold cross validation and lasso statistic MAE is %f", Maek))
message(sprintf("The Mean squared error of the linear regression with k-fold cross validation and lasso fit is %f", MSE))
```
**Results of the Linear Regressions**

For the k-fold cross-validated linear regression with lasso, the R^2 statistic on the test data shows that the model explains the variance ~33.52% of the time which could be better since there is still a large amount of unexplained variance. Also, a high MAE on the test data indicates that the model has a high error level with its predictions. However, the MSE value is lower than the MSE for linear regression with hold-out cross-validation. This model performs far from desirable with a low R^2, and high MAE and MSE. The diagnostic plot RSME vs. Mixing Percentage shows that increasing the shrinkage parameter effects does not really effect the RMSE which is the measure of average deviation from the actual answer. 

For the hold-out cross-validation linear regression, the R^2 statistic shows that the model explains the variance ~31.76% of the time which is not desirable for the same reason that there is still a large portion of unexplained variance that the model does not capture. Also, the MAE value on the test data indicates that the model has a high error level with its predictions. However, the MSE on the test data is noticeably above the MSE on the k-fold cross-validated linear regression. Although, both models preform similarly the k-fold lasso regression model wins out by having slightly better stats in all measured categories. From the diagnostic plots the we can see that the model depicts a linear relationship and no data point influences the model in abnormal way to justify taking it out.

For inference using variable importance measure (VIM) of what predictors are correlated with the response graduation rate, we can see for both the hold-out cross-validation linear regression and the k-fold cross-validated linear regression the most important variables are: Outstate, Apps, Top25perc, Top10perc, and perc.alumni.

<h1 style="font-size: 18px;">**Decision Tree**</h1>

The linear regression models could have preformed poorly due to it the predictors having non-linear relationships making a linear regression approach less effective, so to improve upon the MSE and get more insight into what predictors most influence our response we will fit a decision tree and another with LOOCV cross validation comparing the two. Note: the reason I switched to LOOCV from k-fold is that I got a resampling warning that was a side effect of k-fold which effected the validity of the model, but it should be fine because we will get a lower bias.

```{r}
library(rpart)
library(rpart.plot)
#----------------------------------------------------------------
# Fit a decision tree on the training data
fit.tree = rpart(Grad.Rate ~ ., data=training)

#------------------------------------------------
# preform cross validation to get the best fit
# Define tuning grid
tuneGrid = expand.grid(.cp = seq(0, 1, 0.01))

cv = trainControl(method = "LOOCV")
cv.fit = train(Grad.Rate ~ ., data = training, method = "rpart", trControl = cv, tuneGrid=tuneGrid)
#----------------------------------------------
# Generalization error

pred = predict(fit.tree, testing)
mseTree = mean((pred - testing$Grad.Rate)^2)
MaeTree = mae(pred, testing$Grad.Rate)
RsqTree = R2(pred, testing$Grad.Rate)

pred = predict(cv.fit, testing)
mseCrossTree = mean((pred - testing$Grad.Rate)^2)
MaeCrossTree = mae(pred, testing$Grad.Rate)
RsqCrossTree = R2(pred, testing$Grad.Rate)

message("Metrics for for the decision tree fit with hold-out cross validation")
rpart.plot(fit.tree)
print(varImp(fit.tree))
message(sprintf("Decision tree with hold-out cross validation statistic R^2 is %f", RsqTree))
message(sprintf("Decision tree with hold-out cross validation statistic MAE is %f", MaeTree))
message(sprintf("The Mean squared error of the decision tree with hold-out cross validation is %f", mseTree))

message("Metrics for for the decision tree with cross validation fit")
rpart.plot(cv.fit$finalModel)
print(varImp(cv.fit$finalModel))
message(sprintf("Decision tree with k-fold cross validation statistic R^2 is %f", RsqCrossTree))
message(sprintf("Decision tree with k-fold cross validation statistic MAE is %f", MaeCrossTree))
message(sprintf("The Mean squared error of the k-fold cross validation decision tree is %f", mseCrossTree))
```

**Results of the Decision Trees**

For the LOOCV cross-validated decision tree, the R^2 statistic on the test data shows that the model explains the variance ~28.78% of the time which is poor since there is still a large amount of unexplained variance. Also, the MAE value on the test data indicates that the model has a high error level with its predictions. However, the MSE on the testing data is significantly lower than the linear regression making this model more suitable for predicting graduation rate. This model is a step up from linear regression. Although, it’s still performing far from desirable with an abysmally low R^2, and high MAE and MSE.

For the hold-out cross-validation decision tree, the R^2 statistic shows that the model explains the variance ~28.78% of the time which is the same as the LOOCV cross-validated version and not desirable for the same reason that there is still a large portion of unexplained variance that the model does not capture. Also, the MAE value on the test data indicates that the model has a high level of error with its predictions. However, the MSE on the test data is equivalent to the MSE on the LOOCV cross-validated decision tree. The similarity of statistics between the LOOCV cross-validated and the hold-out cross-validated decision trees could be because of randomness in the data or the training data is not representative of results in the real world.

Another reason and to comment on the actual tree visuals, both are the same most likely because they are working with the same training data making their optimal complexity parameters equal. Therefore, yielding the same tree.

For inference using variable importance measure (VIM) and looking at the decision tree graphs we can see what predictors are correlated with the response graduation rate, we can see for both the hold-out cross-validation decision tree and the LOOCV cross-validated decision tree the most important variables are: Outstate, Apps, Top25perc, Top10perc, and perc.alumni. 


<h1 style="font-size: 18px;">**Random Forest**</h1>

A decision tree compared to a random forest will tend to over fit and not be as accurate, so to maybe get a better model to predict the graduation rate we can fit a random forest.

```{r}
# Already partitioned data for decision tree

library(randomForest)
#----------------------------------------------------------------
# Fit a random forest on the training data
rand.forest = randomForest(Grad.Rate ~ ., data = training, ntree = 500)

#------------------------------------------------
# preform cross validation to get the best fit

cv = trainControl(method = "cv", number = k)
cv.fitRand = train(Grad.Rate ~ ., data = training, method = "rf", trControl = cv, tuneLength=5) # TODO
#TODO

#----------------------------------------------
# Generalization error

pred = predict(rand.forest, testing)
mseRand = mean((pred - testing$Grad.Rate)^2)
MaeRand = mae(pred, testing$Grad.Rate)
RsqRand = R2(pred, testing$Grad.Rate)

pred = predict(cv.fitRand, testing)
mseCrossRand = mean((pred - testing$Grad.Rate)^2)
MaeCrossRand = mae(pred, testing$Grad.Rate)
RsqCrossRand = R2(pred, testing$Grad.Rate)

message(sprintf("Random forest with hold-out cross validation statistics are:"))
varImpPlot(rand.forest)
print(varImp(rand.forest))
message(sprintf("Random forest with hold-out cross validation statistic R^2 is %f", RsqRand))
message(sprintf("Random forest with hold-out cross validation statistic MAE is %f", MaeRand))
message(sprintf("The Mean squared error of the random forest is %f", mseRand))

message(sprintf("Random forest with LOOCV cross validation statistics are:"))
varImpPlot(cv.fitRand$finalModel)
print(varImp(cv.fitRand$finalModel))
message(sprintf("Random forest with LOOCV cross validation statistic R^2 is %f", RsqCrossRand))
message(sprintf("Random forest with LOOCV cross validation statistic MAE is %f", MaeCrossRand))
message(sprintf("The Mean squared error of the k-fold cross validation random forest is %f", mseCrossRand))
```

**Results of the Random Forests**

For the k-fold cross-validated random forest, the R^2 statistic on the test data shows that the model explains the variance ~37.22% of the time which could be better since there is still a large amount of unexplained variance. Also, the MAE value on the test data indicates that the model has a high error level with its predictions. However, the MSE on the testing data is significantly lower than the linear regression and decision tree models, making this model more suitable for predicting graduation rates. This model is the best performing so far in predicting the graduation rate. Although, it still performs far from desirable with an abysmally low R^2, and high MAE and MSE.

For the hold-out cross-validation random forest, the R^2 statistic shows that the model explains the variance ~36.99% of the time which is not desirable for the same reason that there is still a large portion of unexplained variance that the model does not capture. Also, the MAE value on the test data indicates that the model has a high error level with its predictions. However, the MSE on the test data is just above the MSE on the k-fold cross-validated random forest This similarity of statistics between the k-fold cross-validated and the hold-out cross-validation validated random forests could be because of randomness in the data or the training data is not representative of results in the real world.

For inference using variable importance measure (VIM) of what predictors are correlated with the response graduation rate, we can see for both the hold-out cross-validation random forest and the k-fold cross-validated random forest the most important variables are: Outstate, Top25perc, Top10perc, and perc.alumni, Room.Board.

<h1 style="font-size: 30px;">Overall Results, Analysis, and Discussions</h1>

<style>
  table {
    border-collapse: collapse;
    width: 100%;
  }
  th, td {
    text-align: center;
    padding: 8px;
  }
  th {
    background-color: #dddddd;
    font-weight: bold;
  }
  tr:nth-child(even) {
    background-color: #f2f2f2;
  }
</style>

<table>
  <tr>
    <th>Model Name</th>
    <th>R<sup>2</sup></th>
    <th>MSE</th>
    <th>MAE</th>
  </tr>
  <tr>
    <td>Linear Regression Hold-Out Cross-Validation</td>
    <td>0.317613</td>
    <td>205.316608</td>
    <td>10.193009</td>
  </tr>
  <tr>
    <td>Linear Regression k-fold Cross-Validation</td>
    <td>0.335287</td>
    <td>198.085865</td>
    <td>10.106158</td>
  </tr>
  <tr>
    <td>Decision Tree with Hold-Out Cross-Validation</td>
    <td>0.287870</td>
    <td>214.633345</td>
    <td>10.904169</td>
  </tr>
  <tr>
    <td>Decision Tree with LOOCV Cross-Validation</td>
    <td>0.287870</td>
    <td>214.633345</td>
    <td>10.904169</td>
  </tr>
  <tr>
    <td>Random Forest with Hold-Out Cross-Validation</td>
    <td>0.369972</td>
    <td>184.958105</td>
    <td>9.409218</td>
  </tr>
  <tr>
    <td>Random Forest with k-fold Cross-Validation</td>
    <td>0.372286</td>
    <td>184.123877</td>
    <td>9.393987</td>
  </tr>
</table>


Overall, our models were not able to accurately predict graduation rates on the test data. The best performing model was the random forest with k-fold cross-validation, but even this model had a relatively low R2 and high MSE and MAE. suggesting that the available data on graduation rates may not be currently sufficient for making a accurate model meaning further research is needed to identify the most influential variables that impact graduation rates.

Even though, we observed some patterns in the data, it is important to acknowledge the limitations of our analysis. Our sample size may be relatively small, and there may be biases in the data that we were unable to account for. Furthermore, the complex relationships between predictors and the response may require more advanced models we did not consider.

Despite these limitations, we were able to identify several predictors that were important for predicting graduation rates, including Outstate, Apps, Top25perc, Top10perc, and perc.alumni. These predictors may provide valuable insights for educators and policymakers who are seeking to improve graduation rates.

To make better informed decisions future studies could collect additional data on student demographics, SAT or ACT scores, and extracurricular activities. This would help to provide a more comprehensive understanding of the factors that impact graduation rates, and could lead to the development of more accurate predictive models.

<h1 style="font-size: 30px;">Impacts</h1>

This research’s findings suggest that a student's success in college is predetermined by their high school success, rather than the quality of education provided by a particular college. Due to this, there are several potential impacts that could arise if our findings were adopted in the real world.

One impact could be that colleges would focus more on attracting top-performing students from high schools in order to boost their graduation rates leading to a shift in college admissions practices placing more emphasis on factors like high school GPA, rather than on other factors such as extracurricular activities or personal essays.

Additionally, our study suggests that colleges with more funding from alumni donations and higher out-of-state tuition rates tend to have higher graduation rates. If accurate, it could create a more competitive market for college funding and tuition rates between colleges which will all strive to increase their capital in order to attract more top-performing students and boost their graduation rates.

Also, our study suggests that colleges with larger numbers of applications tend to have higher graduation rates. This could lead to a shift in how colleges market themselves and attract applicants, with an emphasis placed on marketing to well preforming high school students in order to increase application, therefore, increasing graduation rates.

Lastly, our study could create a more accountable education system by encouraging the identification of other factors that influence graduation rates, and improving upon them could help to increase graduation rates. For example, if another study motivated by ours shows that students from certain socioeconomic are less likely to graduate from college, colleges could create support programs to help these students succeed.

Overall, our study suggests that the factors that influence graduation rates are complex and not yet fully explored. However, by gaining a better understanding of more factors and complex relationships, we can work towards creating a more equitable and effective education system that helps all students succeed.